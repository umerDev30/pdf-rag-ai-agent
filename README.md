ğŸ§  RAG AI Agent

A Retrieval-Augmented Generation (RAG) system that lets you upload and query PDFs â€” combining semantic search with generative AI answers.
This project uses SentenceTransformer for embeddings, Qdrant (via Docker) as a vector database, and Gemini API (through OpenAI-compatible endpoints) for answer generation.
All dependencies are managed using UV, the next-generation Python package manager.

ğŸš€ Features

ğŸ“„ PDF Ingestion â€” Chunk and embed document content using sentence-transformers.

ğŸ” Semantic Search â€” Store and retrieve document vectors from Qdrant.

ğŸ¤– AI-Powered Answers â€” Use Gemini (via OpenAI endpoints) to generate contextual answers from retrieved text.

âš™ï¸ Event-Driven Workflow â€” Powered by Inngest for scalable function execution.

ğŸŒ FastAPI backend and optional Streamlit frontend for interactive querying.

ğŸ§© Containerized Vector DB â€” Qdrant runs fully in Docker.

ğŸ§° UV-based project â€” Modern, reproducible dependency management.

ğŸ—ï¸ Tech Stack
Component	Tool / Library
Language	Python 3.12+
Package Manager	UV

Web Framework	FastAPI

Vector Database	Qdrant
 (Docker)
Embeddings	SentenceTransformers

LLM	Google Gemini API
 (via OpenAI-compatible endpoints)
Event Handling	Inngest

Frontend	Streamlit
ğŸ§© Project Structure
rag-ai-agent/
â”‚
â”œâ”€â”€ main.py                 # FastAPI app entrypoint
â”œâ”€â”€ data_loader.py          # Loads and chunks PDF content
â”œâ”€â”€ vector_db.py            # Handles Qdrant operations
â”œâ”€â”€ custom_types.py         # Pydantic models for data
â”œâ”€â”€ .env                    # Environment variables
â”œâ”€â”€ Dockerfile / docker-run # Qdrant setup
â”œâ”€â”€ uv.lock / pyproject.toml# Dependency management via UV
â””â”€â”€ README.md               # Project documentation

âš™ï¸ Setup Instructions
1. Clone the repo
git clone https://github.com/yourusername/rag-ai-agent.git
cd rag-ai-agent

2. Start Qdrant in Docker
docker run -d --name qdrantRagDb \
  -p 6333:6333 \
  -v "${PWD}/qdrant_storage:/qdrant/storage" \
  qdrant/qdrant

3. Set up environment variables

Create a .env file in the project root:

GEMINI_API_KEY=your_gemini_key
QDRANT_URL=http://localhost:6333

4. Install dependencies using UV
uv sync

5. Run the FastAPI server
uvicorn main:app --reload

6. (Optional) Run Streamlit UI
streamlit run app.py

ğŸ§  How It Works

PDF Upload â†’ Chunking:
data_loader.py splits documents into overlapping text chunks.

Embedding â†’ Storage:
Chunks are embedded via SentenceTransformer and stored in Qdrant.

User Query â†’ Retrieval:
The query is embedded and matched against stored vectors to retrieve context.

Context â†’ Generation:
Retrieved text is passed to Gemini (via OpenAI endpoints) for generating human-like answers.

Response â†’ User:
The system returns a concise, context-aware answer.

ğŸ§© Example Usage

Query Example:

"What does the document say about revenue growth?"

Response:

"The report indicates that revenue grew by 15% year-over-year due to strong Q2 performance."

ğŸ§± Future Enhancements

ğŸ§¬ Support for multiple document formats (DOCX, TXT)

ğŸ•¸ï¸ Multi-source retrieval (web + PDFs)

â˜ï¸ Cloud deployment (AWS / GCP)

ğŸ§­ Improved ranking using cross-encoders